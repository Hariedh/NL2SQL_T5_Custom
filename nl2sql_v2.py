# -*- coding: utf-8 -*-
"""nl2sql_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wdpa5onxdQRHo4pfCXq3oVwOfsmiteVF
"""

!pip install --upgrade torch torchvision transformers

!pip install -U datasets gradio

import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset
import gradio as gr
import os
from tqdm import tqdm

# 1. WikiSQL Dataset
class WikiSQLDataset(Dataset):
    def __init__(self, split, tokenizer, max_length=512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        dataset = load_dataset('wikisql', split=split)
        self.questions = []
        self.queries = []
        for item in dataset:
            question = item['question']
            sql = item['sql']['human_readable']
            self.questions.append(f"translate to SQL: {question}")
            self.queries.append(sql)

    def __len__(self):
        return len(self.questions)

    def __getitem__(self, idx):
        question = self.questions[idx]
        query = self.queries[idx]
        input_encoding = self.tokenizer(
            question,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        target_encoding = self.tokenizer(
            query,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {
            'input_ids': input_encoding['input_ids'].squeeze(),
            'attention_mask': input_encoding['attention_mask'].squeeze(),
            'labels': target_encoding['input_ids'].squeeze()
        }

# 2. Training Function
def train_model(model, tokenizer, device, output_dir, epochs=3, batch_size=8):
    model.train()
    train_dataset = WikiSQLDataset('train', tokenizer)
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)

    for epoch in range(epochs):
        total_loss = 0
        progress_bar = tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{epochs}")
        for batch in progress_bar:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            optimizer.zero_grad()
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            progress_bar.set_postfix({'loss': loss.item()})
        avg_loss = total_loss / len(train_dataloader)
        print(f"Epoch {epoch+1} Average Loss: {avg_loss:.4f}")

    # Save model after training
    inference = NL2SQLInference(model_dir=output_dir)
    inference.model = model
    inference.tokenizer = tokenizer
    inference.save_model_files(model, tokenizer, output_dir)

# 3. Inference Class
class NL2SQLInference:
    def __init__(self, model_dir="nl2sql_t5_model"):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_dir = model_dir
        self.model = None
        self.tokenizer = None
        self.config = None

    def save_model_files(self, model, tokenizer, output_dir):
        try:
            os.makedirs(output_dir, exist_ok=True)
            torch.save(model.state_dict(), os.path.join(output_dir, 'model.pt'))
            model.config.save_pretrained(output_dir)
            tokenizer.save_pretrained(output_dir)
            print(f"All model files saved successfully to {output_dir}")
            return True
        except Exception as e:
            print(f"Error saving model files: {str(e)}")
            return False

    def load_model_files(self):
        try:
            self.tokenizer = T5Tokenizer.from_pretrained(self.model_dir)
            self.config = T5Config.from_pretrained(self.model_dir)
            self.model = T5ForConditionalGeneration(self.config)
            self.model.load_state_dict(torch.load(
                os.path.join(self.model_dir, 'model.pt'),
                map_location=self.device
            ))
            self.model.to(self.device)
            self.model.eval()
            print(f"Model files loaded successfully from {self.model_dir}")
            return True
        except Exception as e:
            print(f"Error loading model files: {str(e)}")
            return False

    def generate_sql(self, question, max_length=100):
        if not self.model or not self.tokenizer:
            return "Error: Model not loaded"
        input_text = f"translate to SQL: {question}"
        input_ids = self.tokenizer(
            input_text,
            return_tensors='pt',
            padding=True,
            truncation=True
        ).input_ids.to(self.device)
        with torch.no_grad():
            output_ids = self.model.generate(
                input_ids,
                max_length=max_length,
                num_beams=4,
                early_stopping=True
            )
        sql_query = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
        return sql_query

# 4. Gradio Interface
def create_gradio_interface(train_model_flag=False):
    inference = NL2SQLInference()
    output_dir = inference.model_dir
    model_name = 't5-small'

    # If training is requested or model files don't exist
    if train_model_flag or not os.path.exists(os.path.join(output_dir, 'model.pt')):
        print("Training new model or model files not found...")
        tokenizer = T5Tokenizer.from_pretrained(model_name)
        model = T5ForConditionalGeneration.from_pretrained(model_name)
        model.to(inference.device)
        train_model(model, tokenizer, inference.device, output_dir)
    else:
        # Load existing model
        if not inference.load_model_files():
            return "Failed to load model files"

    # Define Gradio interface function
    def process_question(question):
        if not question.strip():
            return "Please enter a valid question"
        return inference.generate_sql(question)

    # Create Gradio interface
    iface = gr.Interface(
        fn=process_question,
        inputs=gr.Textbox(
            lines=2,
            placeholder="Enter your question (e.g., 'Show all employees with salary above 50000')",
            label="Natural Language Question"
        ),
        outputs=gr.Textbox(label="Generated SQL Query"),
        title="NL2SQL: Natural Language to SQL Converter",
        description="Convert natural language questions to SQL queries using T5 model",
        theme=gr.themes.Soft(),
        allow_flagging="never"
    )

    iface.launch()

if __name__ == "__main__":
    # Set train_model_flag=True to force training, False to use existing model if available
    create_gradio_interface(train_model_flag=True)